{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27567,"status":"ok","timestamp":1717279344513,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"aN_d582T3ci3","outputId":"5d812e49-a2d2-4afa-aeef-79468e3e9f8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":303,"status":"ok","timestamp":1717279352653,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"ZazYfOF-3l5C","outputId":"6cadffff-e222-4b81-cfe9-a51eafa62631"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":622,"status":"ok","timestamp":1717279510128,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"Xtvc9aKb6DBj","outputId":"5ed49449-1370-4145-d0e8-9585f1f92702"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1zcb8WXmpzPR8LBuK3Sg-LhTD6MtlZHvp/ECS174Project\n"]}],"source":["%cd drive/Shareddrives/ECS174/ECS174Project\n"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":350,"status":"ok","timestamp":1717279512397,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"JMBtvUnm6K1t","outputId":"a3535d20-c067-45b7-c0d1-23f70b48a7ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[0m\u001b[01;34m'Car Counting.v1i.darknet (1)'\u001b[0m/   \u001b[01;34mimages\u001b[0m/                               with_bus_and_truck.ipynb\n","'Car Counting.v1i.darknet.zip'    vehicle_color_haze_free_model.h5      with_CV.ipynb\n"," ECS174GroupProject.ipynb        \u001b[01;34m'Vehicle Detection.v2-vvvv.darknet'\u001b[0m/   \u001b[01;34myolo\u001b[0m/\n"]}],"source":["%ls"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":656,"status":"ok","timestamp":1717279516407,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"Bp4mBxwBymJ-"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import os\n","\n","names_path = \"./yolo/coco.names\"\n","\n","def predict_cars_buses_trucks(net, yolo_version):\n","    layer_names = net.getLayerNames()\n","\n","    # Handle different formats of getUnconnectedOutLayers output\n","    unconnected_out_layers = net.getUnconnectedOutLayers()\n","    if isinstance(unconnected_out_layers[0], list) or isinstance(unconnected_out_layers[0], np.ndarray):\n","        output_layers = [layer_names[i[0] - 1] for i in unconnected_out_layers]\n","    else:\n","        output_layers = [layer_names[i - 1] for i in unconnected_out_layers]\n","\n","    # Load COCO names\n","    with open(names_path, \"r\") as f:\n","        classes = [line.strip() for line in f.readlines()]\n","\n","    # Function to get image paths from a folder\n","    def get_image_paths(folder):\n","        image_paths = []\n","        for root, dirs, files in os.walk(folder):\n","            for file in files:\n","                if file.endswith(\".jpg\"):\n","                    image_paths.append(os.path.join(root, file))\n","                    # break\n","        return image_paths\n","\n","\n","    test_folder = \"/content/drive/Shareddrives/ECS174/ECS174Project/Vehicle Detection.v2-vvvv.darknet/test\"\n","    # test_folder = \"./Vehicle Detection.v2-vvvv.darknet/test\"\n","\n","    # test_folder = \"./testing\"\n","\n","    # Get all image paths from the test folder\n","    image_paths = get_image_paths(test_folder)\n","\n","    correct = {\"cars\": 0, \"buses\": 0, \"trucks\": 0}\n","\n","    # Process each image\n","    for image_path in image_paths:\n","        ground_truth = {\"num_cars\": 0, \"num_buses\": 0, \"num_trucks\": 0}\n","\n","        # Get ground truth from the image path\n","        with open(image_path.replace(\".jpg\", \".txt\"), \"r\") as f:\n","            # Get the first character of each line\n","            labels = [line[0] for line in f.readlines()]\n","            # print(labels)\n","            for label in labels:\n","                match label:\n","                    # Label 0: Motorcycle\n","                    # Label 1: Sedan\n","                    # Label 2: Suv\n","                    # Label 3: Truck\n","                    # Label 4: Van\n","                    # Label 5: Bus\n","                    # Label 6: SUV\n","                    # Label 7: Pickup\n","\n","                    # TRUCK not used\n","                    # TUCK not used\n","\n","                    # case \"0\": Ignore motorcycles\n","                    case \"1\": ground_truth[\"num_cars\"] += 1\n","                    case \"2\": ground_truth[\"num_cars\"] += 1\n","                    case \"3\": ground_truth[\"num_trucks\"] += 1\n","                    case \"4\": ground_truth[\"num_cars\"] += 1\n","                    case \"5\": ground_truth[\"num_buses\"] += 1\n","                    case \"6\": ground_truth[\"num_cars\"] += 1\n","                    case \"7\": ground_truth[\"num_cars\"] += 1\n","                    # the labels \"TRUNK\" and \"TUCK\" are not used\n","\n","        # print(f\"Ground truth for:, {ground_truth}\")\n","        image = cv2.imread(image_path)\n","\n","        # Check if image is loaded\n","        if image is None:\n","            print(f\"Error: Unable to load image at {image_path}\")\n","            continue\n","\n","        height, width, channels = image.shape\n","\n","        # Detecting objects\n","        blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n","        net.setInput(blob)\n","        outs = net.forward(output_layers)\n","\n","        # Initialize parameters\n","        class_ids = []\n","        confidences = []\n","        boxes = []\n","\n","        # Extract information from the detections\n","        for out in outs:\n","            for detection in out:\n","                scores = detection[5:]\n","                class_id = np.argmax(scores)\n","                confidence = scores[class_id]\n","                if confidence \u003e 0.5:\n","                    # Object detected\n","                    center_x = int(detection[0] * width)\n","                    center_y = int(detection[1] * height)\n","                    w = int(detection[2] * width)\n","                    h = int(detection[3] * height)\n","\n","                    # Rectangle coordinates\n","                    x = int(center_x - w / 2)\n","                    y = int(center_y - h / 2)\n","\n","                    boxes.append([x, y, w, h])\n","                    confidences.append(float(confidence))\n","                    class_ids.append(class_id)\n","\n","        # Non-max suppression\n","        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","\n","        # Count cars, buses, and trucks\n","        car_count = 0\n","        bus_count = 0\n","        truck_count = 0\n","        for i in range(len(boxes)):\n","            if i in indexes:\n","                x, y, w, h = boxes[i]\n","                label = str(classes[class_ids[i]])\n","                if label == \"car\":\n","                    car_count += 1\n","                    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","                    cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","                elif label == \"bus\":\n","                    bus_count += 1\n","                    cv2.rectangle(image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n","                    cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n","                elif label == \"truck\":\n","                    truck_count += 1\n","                    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)\n","                    cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n","\n","        # print(f\"Ground truth: {ground_truth['num_cars']} cars, {ground_truth['num_buses']} buses, {ground_truth['num_trucks']} trucks\")\n","        # print()\n","        # print(f\"Number of cars detected: {car_count}\")\n","        # print(f\"Number of buses detected: {bus_count}\")\n","        # print(f\"Number of trucks detected: {truck_count}\")\n","\n","        # print(f\"Ground truth:\\t{ground_truth['num_cars']} cars,\\t{ground_truth['num_buses']} buses,\\t{ground_truth['num_trucks']} trucks\")\n","        # print(f\"Predicted: \\t{car_count} cars,\\t{bus_count} buses,\\t{truck_count} trucks\")\n","\n","        # give a margin of error of 3\n","        margin_of_error = 3\n","        one_half = 0.5\n","\n","        # if ground_truth['num_cars'] == car_count:\n","        #     correct[\"cars\"] += 1\n","        # elif ground_truth['num_buses'] == bus_count:\n","        #     correct[\"buses\"] += 1\n","        # elif ground_truth['num_trucks'] == truck_count:\n","        #     correct[\"trucks\"] += 1\n","\n","        # Check cars count with a dynamic margin of error\n","        if abs(ground_truth['num_cars'] - car_count) \u003c= (ground_truth['num_cars'] * one_half):\n","            correct[\"cars\"] += 1\n","\n","        # Check buses count with margin of error\n","        elif abs(ground_truth['num_buses'] - bus_count) \u003c= (ground_truth['num_buses'] * one_half):\n","            correct[\"buses\"] += 1\n","\n","        # Check trucks count with margin of error\n","        elif abs(ground_truth['num_trucks'] - truck_count) \u003c= (ground_truth['num_trucks'] * one_half):\n","            correct[\"trucks\"] += 1\n","\n","        # # Convert the image from BGR to RGB\n","        # image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        # # Show the image with detections in the notebook\n","        # plt.figure(figsize=(10, 10))\n","        # plt.imshow(image_rgb)\n","        # plt.axis('off')\n","        # plt.title(f\"Detections in {os.path.basename(image_path)}\")\n","        # plt.show()\n","\n","    print(f\"Performance of YOLOv{yolo_version} on the test set:\")\n","    # Calculate accuracy\n","    for key, value in correct.items():\n","        correct[key] = value / len(image_paths)\n","        print(f\"Percentage accuracy for {key}: {correct[key] * 100:.2f}%\")\n","\n","    # print(f\"Accuracy: {correct}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":179206,"status":"ok","timestamp":1717279701880,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"9eQqMGNLYELF","outputId":"8fa28bc8-2d79-4888-b940-2a907ba38959"},"outputs":[{"name":"stdout","output_type":"stream","text":["Performance of YOLOv2 on the test set:\n","Percentage accuracy for cars: 18.11%\n","Percentage accuracy for buses: 78.19%\n","Percentage accuracy for trucks: 2.06%\n"]}],"source":["# Load YOLOv2\n","weights_path = \"./yolo/yolov2.weights\"\n","config_path = \"./yolo/yolov2.cfg\"\n","\n","net = cv2.dnn.readNet(weights_path, config_path)\n","\n","predict_cars_buses_trucks(net, yolo_version=2)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323285,"status":"ok","timestamp":1717280037465,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"1AwWT9smr5YZ","outputId":"e9d3541e-d2d1-4298-aaa0-8e1fec4e5dfd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Performance of YOLOv3 on the test set:\n","Percentage accuracy for cars: 34.57%\n","Percentage accuracy for buses: 62.55%\n","Percentage accuracy for trucks: 2.06%\n"]}],"source":["# Load YOLOv3\n","weights_path = \"./yolo/yolov3.weights\"\n","config_path = \"./yolo/yolov3.cfg\"\n","\n","net = cv2.dnn.readNet(weights_path, config_path)\n","\n","predict_cars_buses_trucks(net, yolo_version=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":422854,"status":"ok","timestamp":1716841758411,"user":{"displayName":"Brian Li","userId":"08121127222627393431"},"user_tz":420},"id":"wROAsgc2r7W3","outputId":"1547ae76-1aa5-45cd-f26b-c24e65ef7a86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Performance of YOLOv4 on the test set:\n","Percentage accuracy for cars: 46.50%\n","Percentage accuracy for buses: 49.79%\n","Percentage accuracy for trucks: 1.65%\n"]}],"source":["# Load YOLOv4\n","weights_path = \"./yolo/yolov4.weights\"\n","config_path = \"./yolo/yolov4.cfg\"\n","\n","net = cv2.dnn.readNet(weights_path, config_path)\n","\n","predict_cars_buses_trucks(net, yolo_version=4)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":291,"status":"ok","timestamp":1717280060386,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"DQD7UjV8sHrY"},"outputs":[],"source":["def calculate_iou(box1, box2):\n","    x1, y1, w1, h1 = box1\n","    x2, y2, w2, h2 = box2\n","\n","    xi1 = max(x1, x2)\n","    yi1 = max(y1, y2)\n","    xi2 = min(x1 + w1, x2 + w2)\n","    yi2 = min(y1 + h1, y2 + h2)\n","    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n","\n","    box1_area = w1 * h1\n","    box2_area = w2 * h2\n","    union_area = box1_area + box2_area - inter_area\n","\n","    iou = inter_area / union_area\n","    return iou"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1717280063619,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"NqEzHoCg2tqE"},"outputs":[],"source":["def predict_cars_buses_trucks2(net, yolo_version):\n","    layer_names = net.getLayerNames()\n","\n","    # Handle different formats of getUnconnectedOutLayers output\n","    unconnected_out_layers = net.getUnconnectedOutLayers()\n","    if isinstance(unconnected_out_layers[0], list) or isinstance(unconnected_out_layers[0], np.ndarray):\n","        output_layers = [layer_names[i[0] - 1] for i in unconnected_out_layers]\n","    else:\n","        output_layers = [layer_names[i - 1] for i in unconnected_out_layers]\n","\n","    # Load COCO names\n","    with open(names_path, \"r\") as f:\n","        classes = [line.strip() for line in f.readlines()]\n","\n","    # Function to get image paths from a folder\n","    def get_image_paths(folder):\n","        image_paths = []\n","        for root, dirs, files in os.walk(folder):\n","            for file in files:\n","                if file.endswith(\".jpg\"):\n","                    image_paths.append(os.path.join(root, file))\n","        return image_paths\n","\n","    test_folder = \"/content/drive/Shareddrives/ECS174/ECS174Project/Vehicle Detection.v2-vvvv.darknet/test\"\n","\n","    # Get all image paths from the test folder\n","    image_paths = get_image_paths(test_folder)\n","\n","    # Initialize counters\n","    metrics = {\n","        \"cars\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n","        \"buses\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n","        \"trucks\": {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n","    }\n","\n","    # Process each image\n","    for image_path in image_paths:\n","        ground_truth = {\"cars\": [], \"buses\": [], \"trucks\": []}\n","\n","        # Get ground truth from the image path\n","        with open(image_path.replace(\".jpg\", \".txt\"), \"r\") as f:\n","            labels = [line.strip().split() for line in f.readlines()]\n","            for label in labels:\n","                category = int(label[0])\n","                bbox = [float(x) for x in label[1:]]\n","                x, y, w, h = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n","                if category in [1, 2, 4, 6, 7]:  # Cars\n","                    ground_truth[\"cars\"].append([x, y, w, h])\n","                elif category == 5:  # Buses\n","                    ground_truth[\"buses\"].append([x, y, w, h])\n","                elif category == 3:  # Trucks\n","                    ground_truth[\"trucks\"].append([x, y, w, h])\n","\n","        image = cv2.imread(image_path)\n","\n","        # Check if image is loaded\n","        if image is None:\n","            print(f\"Error: Unable to load image at {image_path}\")\n","            continue\n","\n","        height, width, channels = image.shape\n","\n","        # Detecting objects\n","        blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n","        net.setInput(blob)\n","        outs = net.forward(output_layers)\n","\n","        # Initialize parameters\n","        class_ids = []\n","        confidences = []\n","        boxes = []\n","\n","        # Extract information from the detections\n","        for out in outs:\n","            for detection in out:\n","                scores = detection[5:]\n","                class_id = np.argmax(scores)\n","                confidence = scores[class_id]\n","                if confidence \u003e 0.5:\n","                    # Object detected\n","                    center_x = int(detection[0] * width)\n","                    center_y = int(detection[1] * height)\n","                    w = int(detection[2] * width)\n","                    h = int(detection[3] * height)\n","\n","                    # Rectangle coordinates\n","                    x = int(center_x - w / 2)\n","                    y = int(center_y - h / 2)\n","\n","                    boxes.append([x, y, w, h])\n","                    confidences.append(float(confidence))\n","                    class_ids.append(class_id)\n","\n","        # Non-max suppression\n","        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","\n","        detected_objects = {\"cars\": [], \"buses\": [], \"trucks\": []}\n","        for i in range(len(boxes)):\n","            if i in indexes:\n","                label = str(classes[class_ids[i]])\n","                box = boxes[i]\n","                if label == \"car\":\n","                    detected_objects[\"cars\"].append(box)\n","                elif label == \"bus\":\n","                    detected_objects[\"buses\"].append(box)\n","                elif label == \"truck\":\n","                    detected_objects[\"trucks\"].append(box)\n","\n","        # Match detected objects with ground truth using IoU\n","        iou_threshold = 0.5\n","        for label in [\"cars\", \"buses\", \"trucks\"]:\n","            gt_boxes = ground_truth[label]\n","            dt_boxes = detected_objects[label]\n","\n","            matched_gt = []\n","            for gt_box in gt_boxes:\n","                best_iou = 0\n","                best_dt_idx = -1\n","                for idx, dt_box in enumerate(dt_boxes):\n","                    iou = calculate_iou(gt_box, dt_box)\n","                    if iou \u003e best_iou:\n","                        best_iou = iou\n","                        best_dt_idx = idx\n","                if best_iou \u003e= iou_threshold:\n","                    metrics[label][\"TP\"] += 1\n","                    matched_gt.append(gt_box)\n","                    dt_boxes.pop(best_dt_idx)\n","\n","            # Remaining ground truth boxes are false negatives\n","            metrics[label][\"FN\"] += len(gt_boxes) - len(matched_gt)\n","            # Remaining detected boxes are false positives\n","            metrics[label][\"FP\"] += len(dt_boxes)\n","\n","    # Calculate recall, precision, and F1 score\n","    for label, metric in metrics.items():\n","        TP = metric[\"TP\"]\n","        FP = metric[\"FP\"]\n","        FN = metric[\"FN\"]\n","        precision = TP / (TP + FP) if (TP + FP) \u003e 0 else 0\n","        recall = TP / (TP + FN) if (TP + FN) \u003e 0 else 0\n","        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) \u003e 0 else 0\n","        print(f\"Performance of YOLOv{yolo_version} on {label}:\")\n","        print(f\"Precision: {precision:.2f}\")\n","        print(f\"Recall: {recall:.2f}\")\n","        print(f\"F1 Score: {f1_score:.2f}\")"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":289,"status":"ok","timestamp":1717280093452,"user":{"displayName":"Alexander Dsouza","userId":"16549860175879144767"},"user_tz":420},"id":"SPP_oOlj3kxe"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import os\n","\n","names_path = \"./yolo/coco.names\"\n","\n","def calculate_iou(box1, box2):\n","    x1, y1, w1, h1 = box1\n","    x2, y2, w2, h2 = box2\n","\n","    xi1 = max(x1, x2)\n","    yi1 = max(y1, y2)\n","    xi2 = min(x1 + w1, x2 + w2)\n","    yi2 = min(y1 + h1, y2 + h2)\n","    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n","\n","    box1_area = w1 * h1\n","    box2_area = w2 * h2\n","    union_area = box1_area + box2_area - inter_area\n","\n","    iou = inter_area / union_area\n","    return iou\n","\n","def predict_single_image(net, image_path, annotation_path, yolo_version):\n","    layer_names = net.getLayerNames()\n","\n","    # Handle different formats of getUnconnectedOutLayers output\n","    unconnected_out_layers = net.getUnconnectedOutLayers()\n","    if isinstance(unconnected_out_layers[0], list) or isinstance(unconnected_out_layers[0], np.ndarray):\n","        output_layers = [layer_names[i[0] - 1] for i in unconnected_out_layers]\n","    else:\n","        output_layers = [layer_names[i - 1] for i in unconnected_out_layers]\n","\n","    # Load COCO names\n","    with open(names_path, \"r\") as f:\n","        classes = [line.strip() for line in f.readlines()]\n","\n","    # Initialize counters\n","    metrics = {\n","        \"cars\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n","        \"buses\": {\"TP\": 0, \"FP\": 0, \"FN\": 0},\n","        \"trucks\": {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n","    }\n","\n","    ground_truth = {\"cars\": [], \"buses\": [], \"trucks\": []}\n","\n","    # Get ground truth from the annotation file\n","    with open(annotation_path, \"r\") as f:\n","        labels = [line.strip().split() for line in f.readlines()]\n","        for label in labels:\n","            category = int(label[0])\n","            bbox = [float(x) for x in label[1:]]\n","            x, y, w, h = int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])\n","            if category in [1, 2, 4, 6, 7]:  # Cars\n","                ground_truth[\"cars\"].append([x, y, w, h])\n","            elif category == 5:  # Buses\n","                ground_truth[\"buses\"].append([x, y, w, h])\n","            elif category == 3:  # Trucks\n","                ground_truth[\"trucks\"].append([x, y, w, h])\n","\n","    image = cv2.imread(image_path)\n","\n","    # Check if image is loaded\n","    if image is None:\n","        print(f\"Error: Unable to load image at {image_path}\")\n","        return\n","\n","    height, width, channels = image.shape\n","\n","    # Detecting objects\n","    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n","    net.setInput(blob)\n","    outs = net.forward(output_layers)\n","\n","    # Initialize parameters\n","    class_ids = []\n","    confidences = []\n","    boxes = []\n","\n","    # Extract information from the detections\n","    for out in outs:\n","        for detection in out:\n","            scores = detection[5:]\n","            class_id = np.argmax(scores)\n","            confidence = scores[class_id]\n","            if confidence \u003e 0.5:\n","                # Object detected\n","                center_x = int(detection[0] * width)\n","                center_y = int(detection[1] * height)\n","                w = int(detection[2] * width)\n","                h = int(detection[3] * height)\n","\n","                # Rectangle coordinates\n","                x = int(center_x - w / 2)\n","                y = int(center_y - h / 2)\n","\n","                boxes.append([x, y, w, h])\n","                confidences.append(float(confidence))\n","                class_ids.append(class_id)\n","\n","    # Non-max suppression\n","    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n","\n","    detected_objects = {\"cars\": [], \"buses\": [], \"trucks\": []}\n","    for i in range(len(boxes)):\n","        if i in indexes:\n","            label = str(classes[class_ids[i]])\n","            box = boxes[i]\n","            if label == \"car\":\n","                detected_objects[\"cars\"].append(box)\n","                cv2.rectangle(image, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (0, 255, 0), 2)\n","                cv2.putText(image, \"car\", (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","            elif label == \"bus\":\n","                detected_objects[\"buses\"].append(box)\n","                cv2.rectangle(image, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (255, 0, 0), 2)\n","                cv2.putText(image, \"bus\", (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n","            elif label == \"truck\":\n","                detected_objects[\"trucks\"].append(box)\n","                cv2.rectangle(image, (box[0], box[1]), (box[0] + box[2], box[1] + box[3]), (0, 0, 255), 2)\n","                cv2.putText(image, \"truck\", (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n","\n","    # Match detected objects with ground truth using IoU\n","    iou_threshold = 0.5\n","    for label in [\"cars\", \"buses\", \"trucks\"]:\n","        gt_boxes = ground_truth[label]\n","        dt_boxes = detected_objects[label]\n","\n","        matched_gt = []\n","        for gt_box in gt_boxes:\n","            best_iou = 0\n","            best_dt_idx = -1\n","            for idx, dt_box in enumerate(dt_boxes):\n","                iou = calculate_iou(gt_box, dt_box)\n","                if iou \u003e best_iou:\n","                    best_iou = iou\n","                    best_dt_idx = idx\n","            if best_iou \u003e= iou_threshold:\n","                metrics[label][\"TP\"] += 1\n","                matched_gt.append(gt_box)\n","                dt_boxes.pop(best_dt_idx)\n","\n","        # Remaining ground truth boxes are false negatives\n","        metrics[label][\"FN\"] += len(gt_boxes) - len(matched_gt)\n","        # Remaining detected boxes are false positives\n","        metrics[label][\"FP\"] += len(dt_boxes)\n","\n","    # Calculate recall, precision, and F1 score\n","    for label, metric in metrics.items():\n","        TP = metric[\"TP\"]\n","        FP = metric[\"FP\"]\n","        FN = metric[\"FN\"]\n","        precision = TP / (TP + FP) if (TP + FP) \u003e 0 else 0\n","        recall = TP / (TP + FN) if (TP + FN) \u003e 0 else 0\n","        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) \u003e 0 else 0\n","        print(f\"Performance of YOLOv{yolo_version} on {label}:\")\n","        print(f\"Precision: {precision:.2f}\")\n","        print(f\"Recall: {recall:.2f}\")\n","        print(f\"F1 Score: {f1_score:.2f}\")\n","\n","    # Show the image with detections\n","    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    plt.figure(figsize=(10, 10))\n","    plt.imshow(image_rgb)\n","    plt.axis('off')\n","    plt.title(f\"Detections with YOLOv{yolo_version}\")\n","    plt.show()\n","\n","# Example usage:\n","# net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n","# image_path = \"path/to/your/image.jpg\"\n","# annotation_path = \"path/to/your/annotation.txt\"\n","# predict_single_image(net, image_path, annotation_path, 3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Kirse3Lb3rn-"},"outputs":[],"source":["net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n","image_path = \"drive/Shareddrives/ECS174/ECS174Project/Vehicle Detection.v2-vvvv.darknet/2_jpg.rf.e441efe862927285ed5dc190e0b11ab8.jpg\"\n","annotation_path = \"drive/Shareddrives/ECS174/ECS174Project/Vehicle Detection.v2-vvvv.darknet/2_jpg.rf.e441efe862927285ed5dc190e0b11ab8.txt\"\n","predict_single_image(net, image_path, annotation_path, 3)\n"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}